{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8e652f-d599-4ba9-879d-e53accfe9472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozpoczynanie treningu...\n",
      "Epoch 0, Loss: 0.693978\n",
      "Epoch 1, Loss: 0.678093\n",
      "Epoch 2, Loss: 0.614516\n",
      "Epoch 3, Loss: 0.445526\n",
      "Epoch 4, Loss: 0.283458\n",
      "Epoch 5, Loss: 0.167323\n",
      "Epoch 6, Loss: 0.073782\n",
      "Epoch 7, Loss: 0.024623\n",
      "Epoch 8, Loss: 0.005985\n",
      "Epoch 9, Loss: 0.001163\n",
      "Epoch 10, Loss: 0.000202\n",
      "Epoch 11, Loss: 0.000036\n",
      "Epoch 12, Loss: 0.000007\n",
      "Epoch 13, Loss: 0.000001\n",
      "Epoch 14, Loss: 0.000000\n",
      "Epoch 15, Loss: 0.000000\n",
      "Epoch 16, Loss: 0.000000\n",
      "Epoch 17, Loss: 0.000000\n",
      "Epoch 18, Loss: 0.000000\n",
      "Epoch 19, Loss: 0.000000\n",
      "\n",
      "Wyniki testowe:\n",
      "Test 1:   94367 +  231698 =   326065 | Sieć:   326065 (OK)\n",
      "Test 2:  180392 +   85253 =   265645 | Sieć:   265645 (OK)\n",
      "Test 3:  188528 +  183772 =   372300 | Sieć:   372300 (OK)\n",
      "Test 4:   24015 +  190568 =   214583 | Sieć:   214583 (OK)\n",
      "Test 5:   99818 +  222546 =   322364 | Sieć:   322364 (OK)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "np.random.seed(seed=1)\n",
    "\n",
    "# Przygotowanie danych (Suma 18-bitowa) \n",
    "# sequence_len = bits + 1 (18 bitów + ewentualny bit przeniesienia)\n",
    "bits = 18\n",
    "sequence_len = bits + 1 \n",
    "nb_train = 1000 \n",
    "\n",
    "def create_dataset(nb_samples, seq_len):\n",
    "    max_int = 2**(seq_len-1) - 1\n",
    "    format_str = '{:0' + str(seq_len) + 'b}'\n",
    "    X = np.zeros((nb_samples, seq_len, 2))\n",
    "    T = np.zeros((nb_samples, seq_len, 1))\n",
    "    for i in range(nb_samples):\n",
    "        nb1 = np.random.randint(0, max_int)\n",
    "        nb2 = np.random.randint(0, max_int)\n",
    "        X[i,:,0] = list(reversed([int(b) for b in format_str.format(nb1)]))\n",
    "        X[i,:,1] = list(reversed([int(b) for b in format_str.format(nb2)]))\n",
    "        T[i,:,0] = list(reversed([int(b) for b in format_str.format(nb1 + nb2)]))\n",
    "    return X, T\n",
    "\n",
    "X_train, T_train = create_dataset(nb_train, sequence_len)\n",
    "\n",
    "# 2. Komponenty Sieci\n",
    "\n",
    "class TensorLinear(object):\n",
    "    def __init__(self, n_in, n_out, tensor_order, W=None, b=None):\n",
    "        a = np.sqrt(6.0 / (n_in + n_out))\n",
    "        self.W = np.random.uniform(-a, a, (n_in, n_out)) if W is None else W\n",
    "        self.b = np.zeros((n_out)) if b is None else b\n",
    "        self.bpAxes = tuple(range(tensor_order-1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return np.tensordot(X, self.W, axes=((-1),(0))) + self.b\n",
    "\n",
    "    def backward(self, X, gY):\n",
    "        gW = np.tensordot(X, gY, axes=(self.bpAxes, self.bpAxes))\n",
    "        gB = np.sum(gY, axis=self.bpAxes)\n",
    "        gX = np.tensordot(gY, self.W.T, axes=((-1),(0)))  \n",
    "        return gX, gW, gB\n",
    "\n",
    "class LogisticClassifier(object):\n",
    "    def forward(self, X):\n",
    "        return 1. / (1. + np.exp(-np.clip(X, -15, 15))) \n",
    "    \n",
    "    def backward(self, Y, T):\n",
    "        return (Y - T) / (Y.shape[0] * Y.shape[1])\n",
    "    \n",
    "    def loss(self, Y, T):\n",
    "        return -np.mean((T * np.log(Y + 1e-10)) + ((1-T) * np.log(1-Y + 1e-10)))\n",
    "\n",
    "class TanH(object):\n",
    "    def forward(self, X): return np.tanh(X) \n",
    "    def backward(self, Y, output_grad): return (1.0 - (Y**2)) * output_grad\n",
    "\n",
    "class RecurrentStateUpdate(object):\n",
    "    def __init__(self, nbStates, W, b):\n",
    "        self.linear = TensorLinear(nbStates, nbStates, 2, W, b)\n",
    "        self.tanh = TanH()\n",
    "\n",
    "    def forward(self, Xk, Sk):\n",
    "        return self.tanh.forward(Xk + self.linear.forward(Sk))\n",
    "    \n",
    "    def backward(self, Sk0, Sk1, output_grad):\n",
    "        gZ = self.tanh.backward(Sk1, output_grad)\n",
    "        gSk0, gW, gB = self.linear.backward(Sk0, gZ)\n",
    "        return gZ, gSk0, gW, gB\n",
    "\n",
    "class RecurrentStateUnfold(object):\n",
    "    def __init__(self, nbStates, nbTimesteps):\n",
    "        a = np.sqrt(6. / (nbStates * 2))\n",
    "        self.W = np.random.uniform(-a, a, (nbStates, nbStates))\n",
    "        self.b = np.zeros((self.W.shape[0]))\n",
    "        self.S0 = np.zeros(nbStates)\n",
    "        self.nbTimesteps = nbTimesteps\n",
    "        self.stateUpdate = RecurrentStateUpdate(nbStates, self.W, self.b)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        S = np.zeros((X.shape[0], X.shape[1]+1, self.W.shape[0]))\n",
    "        S[:,0,:] = self.S0\n",
    "        for k in range(self.nbTimesteps):\n",
    "            S[:,k+1,:] = self.stateUpdate.forward(X[:,k,:], S[:,k,:])\n",
    "        return S\n",
    "    \n",
    "    def backward(self, X, S, gY):\n",
    "        gSk = np.zeros_like(gY[:,self.nbTimesteps-1,:])\n",
    "        gZ = np.zeros_like(X)\n",
    "        gWSum, gBSum = np.zeros_like(self.W), np.zeros_like(self.b)\n",
    "        for k in range(self.nbTimesteps-1, -1, -1):\n",
    "            gSk += gY[:,k,:]\n",
    "            gZ[:,k,:], gSk, gW, gB = self.stateUpdate.backward(S[:,k,:], S[:,k+1,:], gSk)\n",
    "            gWSum += gW\n",
    "            gBSum += gB\n",
    "        return gZ, gWSum, gBSum, np.sum(gSk, axis=0)\n",
    "\n",
    "\n",
    "class RnnBinaryAdder(object):\n",
    "    def __init__(self, nb_of_inputs, nb_of_outputs, nb_of_states, sequence_len):\n",
    "        self.tensorInput = TensorLinear(nb_of_inputs, nb_of_states, 3)\n",
    "        self.rnnUnfold = RecurrentStateUnfold(nb_of_states, sequence_len)\n",
    "        self.tensorOutput = TensorLinear(nb_of_states, nb_of_outputs, 3)\n",
    "        self.classifier = LogisticClassifier()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        recIn = self.tensorInput.forward(X)\n",
    "        S = self.rnnUnfold.forward(recIn)\n",
    "        Z = self.tensorOutput.forward(S[:,1:,:])\n",
    "        Y = self.classifier.forward(Z)\n",
    "        return recIn, S, Z, Y\n",
    "    \n",
    "    def backward(self, X, Y, recIn, S, T):\n",
    "        gZ = self.classifier.backward(Y, T)\n",
    "        gRecOut, gWout, gBout = self.tensorOutput.backward(S[:,1:,:], gZ)\n",
    "        gRnnIn, gWrec, gBrec, gS0 = self.rnnUnfold.backward(recIn, S, gRecOut)\n",
    "        _, gWin, gBin = self.tensorInput.backward(X, gRnnIn)\n",
    "        return gWout, gBout, gWrec, gBrec, gWin, gBin, gS0\n",
    "    \n",
    "    def getOutput(self, X): return self.forward(X)[3]\n",
    "    def getBinaryOutput(self, X): return np.around(self.getOutput(X))\n",
    "    def loss(self, Y, T): return self.classifier.loss(Y, T)\n",
    "    \n",
    "    def getParamGrads(self, X, T):\n",
    "        recIn, S, Z, Y = self.forward(X)\n",
    "        gWout, gBout, gWrec, gBrec, gWin, gBin, gS0 = self.backward(X, Y, recIn, S, T)\n",
    "        return [g for g in itertools.chain(\n",
    "                np.nditer(gS0),\n",
    "                np.nditer(gWin),\n",
    "                np.nditer(gBin),\n",
    "                np.nditer(gWrec),\n",
    "                np.nditer(gBrec),\n",
    "                np.nditer(gWout),\n",
    "                np.nditer(gBout))]\n",
    "\n",
    "    def get_params_iter(self):\n",
    "        return itertools.chain(\n",
    "            np.nditer(self.rnnUnfold.S0, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorInput.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.W, op_flags=['readwrite']),\n",
    "            np.nditer(self.rnnUnfold.b, op_flags=['readwrite']),\n",
    "            np.nditer(self.tensorOutput.W, op_flags=['readwrite']), \n",
    "            np.nditer(self.tensorOutput.b, op_flags=['readwrite']))\n",
    "\n",
    "# 4. Trening z RMSProp i Momentum \n",
    "\n",
    "nb_of_states = 12 \n",
    "RNN = RnnBinaryAdder(2, 1, nb_of_states, sequence_len)\n",
    "\n",
    "lmbd, learning_rate, momentum_term = 0.5, 0.01, 0.9\n",
    "mb_size, epochs = 100, 20\n",
    "\n",
    "nbParameters = sum(1 for _ in RNN.get_params_iter())\n",
    "maSquare = [0.0] * nbParameters\n",
    "Vs = [0.0] * nbParameters\n",
    "losses = []\n",
    "\n",
    "print(\"Rozpoczynanie treningu...\")\n",
    "for epoch in range(epochs):\n",
    "    for mb in range(nb_train // mb_size):\n",
    "        X_mb = X_train[mb*mb_size:(mb+1)*mb_size]\n",
    "        T_mb = T_train[mb*mb_size:(mb+1)*mb_size]\n",
    "        \n",
    "        V_tmp = [v * momentum_term for v in Vs]\n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()): P += V_tmp[pIdx]\n",
    "        \n",
    "        backprop_grads = RNN.getParamGrads(X_mb, T_mb)    \n",
    "        \n",
    "        for pIdx, P in enumerate(RNN.get_params_iter()):\n",
    "            maSquare[pIdx] = lmbd * maSquare[pIdx] + (1-lmbd) * backprop_grads[pIdx]**2\n",
    "            pGradNorm = (learning_rate * backprop_grads[pIdx]) / (np.sqrt(maSquare[pIdx]) + 1e-7)\n",
    "            Vs[pIdx] = V_tmp[pIdx] - pGradNorm     \n",
    "            P -= pGradNorm\n",
    "            \n",
    "    current_loss = RNN.loss(RNN.getOutput(X_train[:200]), T_train[:200])\n",
    "    losses.append(current_loss)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {current_loss:.6f}\")\n",
    "\n",
    "\n",
    "print(\"\\nWyniki testowe:\")\n",
    "X_t, T_t = create_dataset(5, sequence_len)\n",
    "Y_t = RNN.getBinaryOutput(X_t)\n",
    "for i in range(5):\n",
    "    x1 = int(\"\".join(reversed([str(int(d)) for d in X_t[i,:,0]])), 2)\n",
    "    x2 = int(\"\".join(reversed([str(int(d)) for d in X_t[i,:,1]])), 2)\n",
    "    target = int(\"\".join(reversed([str(int(d)) for d in T_t[i,:,0]])), 2)\n",
    "    pred = int(\"\".join(reversed([str(int(d)) for d in Y_t[i,:,0]])), 2)\n",
    "    status = \"OK\" if target == pred else \"BŁĄD\"\n",
    "    print(f\"Test {i+1}: {x1:7} + {x2:7} = {target:8} | Sieć: {pred:8} ({status})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc3c19-8a43-4351-a2a0-618343636f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
